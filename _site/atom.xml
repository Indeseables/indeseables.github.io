<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>Indeseabλes</title>
 <link href="http://indeseables.github.io/atom.xml" rel="self"/>
 <link href="http://indeseables.github.io/"/>
 <updated>2016-04-02T05:44:08+02:00</updated>
 <id>http://indeseables.github.io</id>
 <author>
   <name>Indeseables</name>
   <email>indeseables.git@gmail.com</email>
 </author>

 
 <entry>
   <title>Analisis de malware sirio</title>
   <link href="http://indeseables.github.io/2016/03/29/analisis-de-malware-sirio/"/>
   <updated>2016-03-29T00:00:00+02:00</updated>
   <id>http://indeseables.github.io/2016/03/29/analisis-de-malware-sirio</id>
   <content type="html">&lt;p&gt;Ayer estuve varias horas recolectando samples para hacer mi primera entrada en el blog sobre el análisis de malware pero sólo vi keyloggers y servidores de DarkComet cifrados con crypters en .NET, así que buscando en google encontré &lt;a href=&quot;http://syrianmalware.com/&quot;&gt;esta página&lt;/a&gt; y aunque el último sample es de 2014 me pareció curiosa.&lt;/p&gt;

&lt;p&gt;El primer análisis es del primer sample de la web (a8ef5ccebd2e3babdd243a2861673c26). Quiero aclarar que no soy un experto en análisis así que mis procedimientos quizá no sean los más correctos.&lt;/p&gt;

&lt;h2 id=&quot;a8ef5ccebd2e3babdd243a2861673c26&quot;&gt;a8ef5ccebd2e3babdd243a2861673c26&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Filename: news.exe&lt;/li&gt;
  &lt;li&gt;MD5 Hash: a8ef5ccebd2e3babdd243a2861673c26&lt;/li&gt;
  &lt;li&gt;Filesize: 114K&lt;/li&gt;
  &lt;li&gt;Analysis or Media:
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://docs.google.com/document/d/1LCfzNKtDwBn2BqKo_rvgCxMug8Ow-lZMBT89UFayAhI/edit?usp=sharing&quot;&gt;Quick Analysis&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.virustotal.com/en/file/039058cd0f349c8987a4a61a3de12660b78007235126ee75228933fda2343e4f/analysis/1412724921/&quot;&gt;VirusTotal Results&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://malwr.com/analysis/YTE3YWRlNjZmMjlhNGNjZmI1YmRjNDQ3YmI1MjAyZjQ/&quot;&gt;Malwr Analysis&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Un primer vistazo rápido con CFF Explorer nos muestra que es un ejecutable .NET así que veremos si se puede decompilar. Para ello usaré ILSpy ya que es el que mejor me ha funcionado en la máquina virtual. Al decompilar el ejecutable y abrimos cualquier clase vemos que parece ser que está ofuscado.
&lt;img src=&quot;https://i.gyazo.com/f4848a908ff3b9c0e8d80aa21a055a82.png&quot; alt=&quot;FIG1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Suerte que tenemos &lt;a href=&quot;https://github.com/0xd4d/de4dot&quot;&gt;de4dot&lt;/a&gt;. Normalmente uso un par de veces de4dot por el ejecutable por si las moscas y en este caso parece ser que acerté.
&lt;img src=&quot;https://i.gyazo.com/5d56ba4ffb3bcc74e430d09e881b84ae.png&quot; alt=&quot;FIG2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;En una de las clases vemos una variable bastante larga cifrada con Base64, seguramente un ejecutable, pero me entra la curiosidad que empiece por &amp;lt;COMPRESSED&amp;gt; y acabe en &amp;lt;/COMPRESSED&amp;gt; así que hay que buscar en el resto de clases alguna referencia a una función de compresión.
&lt;img src=&quot;https://i.gyazo.com/8f663ac394688c72cea81f92a0c918a7.png&quot; alt=&quot;FIG3&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Encuentro a la primera la clase encargada de la compresión al ver que hay una llamada ClsCompressedString (¿seré un genio?) y una rápida búsqueda en Google me lleva a &lt;a href=&quot;http://www.codeproject.com/Articles/27396/Easy-String-Compression-and-Encryption&quot;&gt;esta página&lt;/a&gt; donde seguramente el súper hacker habrá sacado el código. Creo un nuevo proyecto en VB.NET y uso la clase para descomprimir y descifrar la string usando la misma función que usa el crypter en una de sus clases. El archivo cifrado se puede descifrar con la clase clsCompressedString comentada antes, luego convirtiendo el resultado (string hexadecimal) a una array de bytes y por último descomprimiendo con GZip. El resultado es (otro) ejecutable .NET, el cual, al decompilarlo (con &lt;a href=&quot;https://github.com/0xd4d/dnSpy&quot;&gt;dnSpy&lt;/a&gt;) parece ser un servidor de NjRAT 0.6.4 que conecta al DNS aliahmahhmod.zapto.org, offline en el momento de escribir el post.&lt;/p&gt;

&lt;h2 id=&quot;e1d84b350c1465bb4c4c77b1bcec&quot;&gt;7263e1d84b350c1465bb4c4c77b1bcec&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Filename: برنامج الفيش.exe&lt;/li&gt;
  &lt;li&gt;MD5 Hash: 7263e1d84b350c1465bb4c4c77b1bcec&lt;/li&gt;
  &lt;li&gt;Filesize: 8.6M
Analysis or Media:
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://docs.google.com/document/d/1DZvHVn1r_EOj53eYKhQ2jO4mHjR5oX1JbLoxqn-8VpM/edit?usp=sharing&quot;&gt;Quick Analysis&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.virustotal.com/en/file/20c52aedfe32d4a71bf5e3353cf3999ea95b3a43c7e40c8820b1b7d24b9e6cb5/analysis/1410845347/&quot;&gt;VirusTotal Results&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;CFF Explorer nos revela que es un ejecutable .NET (qué sorpresa) así que lo pasamos por de4dot, que indicará que no está ofuscado (unknown ofuscator) por eso lo decompilaré directamente. En ILSpy vemos sólo hay dos clases pero buscando en una de ellas (frmMain) vemos que en el método frmMain_Load (que se ejecutará al abrirse el formulario) hay un código para guardar un archivo en AppData con nombre ‘sql’ (niños malos).
&lt;img src=&quot;https://i.gyazo.com/c2797d41f78a506de6c48c1512ab3048.png&quot; alt=&quot;FIG4&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ILSpy no me deja guardar el recurso así que lo abrí desde dnSpy y así lo pude guardar. Al examinar el archivo con CFF Explorer me llevé una enorme sorpresa el ver que era… otro ejecutable .NET. Al decompilar dicho ejecutable veo que sólo tiene una clase llamada Downloader así que tendré que analizarlo durante horas para saber cual es su propósito.
&lt;img src=&quot;https://i.gyazo.com/1154346a2b7dc2ab49e3460c3d835fa7.png&quot; alt=&quot;FIG5&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Ambos archivos no están disponibles. Un punto para ellos.&lt;/p&gt;

&lt;h2 id=&quot;bf01f67db4a5e8e6174b066775eae0&quot;&gt;28bf01f67db4a5e8e6174b066775eae0&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Filename: psiphon.exe&lt;/li&gt;
  &lt;li&gt;MD5 Hash: 28bf01f67db4a5e8e6174b066775eae0&lt;/li&gt;
  &lt;li&gt;Filesize: 1.4M&lt;/li&gt;
  &lt;li&gt;Analysis or Media:
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://citizenlab.org/2014/03/maliciously-repackaged-psiphon/&quot;&gt;Maliciously Repackaged Psiphon Found&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.virustotal.com/en/file/1182ffd81b4ee9bed90ca490ca5bb258e19cce68175d1a69f054030db1075df6/analysis/&quot;&gt;VirusTotal Results&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Tercer round. CFF Explorer nos muestra que es, increíblemente, un ejecutable .NET. Ya sabéis la historia: de4dot, ILSpy/dnSpy, buscar, etc. Este ejecutable sólo tiene una clase llamada Windows que no es más que un formulario que droppea dos ejecutables desde recursos llamados server.exe (muy original…) y psiphon3.exe.&lt;/p&gt;

&lt;p&gt;El ejecutable psiphon3 me pareció bastante “profesional” así que lo busqué en Google y encontré esta información en la misma &lt;a href=&quot;https://s3.amazonaws.com/f58p-mqce-k1yj/en.html&quot;&gt;página oficial&lt;/a&gt;:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;What is Psiphon 3?&lt;/p&gt;

  &lt;p&gt;Psiphon 3 is a circumvention tool from Psiphon Inc. that utilizes VPN, SSH and HTTP Proxy technology to provide you with uncensored access to Internet content. Your Psiphon 3 client will automatically learn about new access points to maximize your chances of bypassing censorship.&lt;/p&gt;

  &lt;p&gt;Psiphon 3 is designed to provide you with open access to online content. Psiphon does not increase your online privacy, and &amp;gt;should not be considered or used as an online security tool.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Así que supongo que será una herramienta auxiliar para el otro ejecutable (server.exe) que analizaré ahora.&lt;/p&gt;

&lt;p&gt;Este ejecutable (.NET como no) está ofuscado con Crypto Ofuscator. Una vez decompilado… sigue estando ofuscado. de4dot no puede limpiar el ejecutable así que al mirar el código fuente no entiendo nada de nada. Tendré que tirar de análisis dinámico. Para ello usaré &lt;a href=&quot;https://technet.microsoft.com/en-us/sysinternals/tcpview.aspx&quot;&gt;TcpView&lt;/a&gt; y &lt;a href=&quot;http://www.sandboxie.com/&quot;&gt;Sandboxie&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Sandboxie nos revela que el ejecutable se autocopia en AppData con el nombre Explorer.exe y en la carpeta de Autoinicio con nombre chrome.exe. TcpView nos muestra que intenta conectarse a 31.9.48.141:1960.&lt;/p&gt;

&lt;h2 id=&quot;conclusin&quot;&gt;Conclusión&lt;/h2&gt;
&lt;p&gt;Durante estos pequeños análisis he sacado tres conclusiones:
* Los sirios aman .NET.
* Los sirios realmente aman .NET.
* Sus técnicas son bastante simples. Se sirven de crypters .NET para RATs públicos (NjRAT).&lt;/p&gt;

&lt;h2 id=&quot;contacto&quot;&gt;Contacto&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Email: indeseables [at] gmail [dot] com&lt;/li&gt;
  &lt;li&gt;Twitter: &lt;a href=&quot;http://twitter.com/&quot;&gt;@indeseables&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Issues: &lt;a href=&quot;https://github.com/Indeseables/indeseables.github.io/issues&quot;&gt;#Issues&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Autor:&lt;/strong&gt; &lt;a href=&quot;https://github.com/blau72&quot;&gt;&lt;em&gt;Blau&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Si tienes alguna duda, comentario o sugerencia, notifícanos a través de &lt;a href=&quot;mailto:indeseables.git@gmail.com&quot;&gt;indeseables.git@gmail.com&lt;/a&gt;
 y si te ha gustado la entrada puedes &lt;a href=&quot;https://twitter.com/intent/tweet?url=http://indeseables.github.io/2016/03/29/analisis-de-malware-sirio/&amp;amp;text=Analisis de malware sirio&amp;amp;via=indeseables!&quot; target=&quot;_blank&quot;&gt; compartirla!.&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;
</content>
 </entry>
 
 <entry>
   <title>Odisea Funcional (Parte 1)</title>
   <link href="http://indeseables.github.io/2016/03/28/odisea-funcional-parte1/"/>
   <updated>2016-03-28T00:00:00+02:00</updated>
   <id>http://indeseables.github.io/2016/03/28/odisea-funcional-parte1</id>
   <content type="html">&lt;p&gt;Un título muy acertado pues estudiar el paradigma funcional es cuanto menos una odisea, &lt;strong&gt;preciosa o no dependiendo de a quíen preguntes.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Me apoyaré en apuntes de la universidad en la que estudio, luego el lenguaje utilizado será el mismo, &lt;strong&gt;C#, funcional no puro&lt;/strong&gt;, pero trataré de comentar &lt;strong&gt;diferencias con un funcional puro como Haskell&lt;/strong&gt; para que se entiendan mejor los conceptos estudiados.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Se pretende&lt;/strong&gt; que en esta odisea naveguemos entre los siguientos puntos:&lt;br /&gt;
* &lt;strong&gt;Cálculo lambda&lt;/strong&gt;
* &lt;strong&gt;Funciones como entidades de primer orden&lt;/strong&gt;
* Cláusulas&lt;br /&gt;
* Currificación&lt;br /&gt;
* Aplicación Parcial&lt;br /&gt;
* Continuaciones&lt;br /&gt;
* Evaluación Perezosa&lt;br /&gt;
* Transparencia Referencial&lt;br /&gt;
* Pattern Matching&lt;br /&gt;
* Funciones de Orden Superior&lt;br /&gt;
* Listas por comprensión&lt;/p&gt;

&lt;p&gt;Recalco el &lt;strong&gt;&lt;em&gt;“Se pretende”&lt;/em&gt;&lt;/strong&gt; porque no estoy seguro de si podre escribir sobre todo sin que me demoré demasiado, el motivo principal de escribir esta odisea es que me sirva a la vez de estudio.&lt;/p&gt;

&lt;p&gt;En esta primera parte se estudiará el contenido &lt;strong&gt;&lt;em&gt;más teorico&lt;/em&gt;&lt;/strong&gt; puesto que abarca una introducción a la programación funcional y un breve estudio de la base desde la que empezó todo, el &lt;strong&gt;lambda cálculo&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;PD&lt;/strong&gt;: Intentaré que esta primera parte sea lo menos aburrida posible, pero no hay todo sin nada, y en este caso el contenido que leereis será prácticamente una copia, síntesis o como lo queráis llamar de los apuntes de mi universidad. (Que por cierto, no se si puedo adjuntar en formato PDF al final del post, por lo que por si acaso no se me permite, no lo haré)&lt;/p&gt;

&lt;h2 id=&quot;qu-es-el-paradigma-funcional&quot;&gt;¿Qué es el paradigma funcional?&lt;/h2&gt;
&lt;blockquote&gt;
  &lt;p&gt;Paradigma declarativo basado en la utilización de funciones que manejan &lt;strong&gt;datos inmutables&lt;/strong&gt;
* Los datos nunca se modifican
* En lugar de cambiar un dato, se llama a una función que devuelve el dato modificado sin modificar el original&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Un programa se define mediante un conjunto de funciones invocándose entre sí&lt;br /&gt;
* Las funciones no generan efectos secundarios
* El valor de una expresión depende únicamente de los valores de los parámetros
* (Devolviendo siempre el mismo valor en función de éstos -&amp;gt; Paradigma funcional puro)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;lambda-clculo-o-clculo-lambda&quot;&gt;Lambda cálculo o cálculo lambda&lt;/h2&gt;
&lt;blockquote&gt;
  &lt;p&gt;Es un sistema formal basado en la definición de funciones (abstracción) y su aplicación (invocación), origen de la programación funcional cuando fue definido por &lt;strong&gt;Church y Kleene&lt;/strong&gt; alla por los años 30.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;Hace uso exhaustivo de la recursión&lt;/li&gt;
  &lt;li&gt;Se considera el lenguaje más pequeño universal de computación&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;&lt;em&gt;Frecuentemente utilizado por investigadores y diseñadores de lenguajes&lt;/em&gt;&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;expresiones-lambda&quot;&gt;Expresiones lambda&lt;/h3&gt;
&lt;p&gt;Se definen como:
* Una abstracción lambda &lt;strong&gt;&lt;em&gt;λx.M (M, N, M1, M2, …)&lt;/em&gt;&lt;/strong&gt;
donde &lt;strong&gt;x&lt;/strong&gt; es una &lt;strong&gt;variable/parámetro&lt;/strong&gt;, y &lt;strong&gt;M&lt;/strong&gt; es una &lt;strong&gt;λ-expresión/cuerpo de la función&lt;/strong&gt;
* Una aplicación &lt;strong&gt;M N&lt;/strong&gt; , donde &lt;strong&gt;M y N son λ-expresiones&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Ejemplos de abstracciones:
* Función identidad &lt;strong&gt;f(x) = x&lt;/strong&gt; puede representarse como: &lt;strong&gt;λx.x&lt;/strong&gt;
* Función doble g(x) = x+x puede representarse como: &lt;strong&gt;λx.x+x&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;PD: No tendría que ser necesario decir que el símbolo “+” podría escribirse en código lambda, pero con fines académicos vamos a considerar las operaciones básicas como “disponibles” en nuestro “intérprete de lambda cálculo”&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;reduccin---aplicacin&quot;&gt;Reducción-β ó Aplicación&lt;/h3&gt;
&lt;p&gt;La aplicación representa su invocación ( la de la función ) y se define del siguiente modo:
* &lt;code class=&quot;highlighter-rouge&quot;&gt;(λx.M)N -&amp;gt; M[x:=N]   (o M[N/x])&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Donde X es la variable, M y N son las λ-expresiones y &lt;strong&gt;M[x:=N] (o M[N/x]) representa la expresión lambda M, donde todas las x son sustituidas por la expresión lambda N.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A esta sustitución se le denomina reducción-β&lt;/strong&gt;, uds. más que nadie deben saber que el mundo de la ciencia esta rodeado por terminología confusa para de alguna manera espantar al que no se atreva a leer un párrafo entero.&lt;/p&gt;

&lt;p&gt;Ejemplos:
* &lt;code class=&quot;highlighter-rouge&quot;&gt;(λx.x + x)3 -&amp;gt; 3 + 3&lt;/code&gt;
* &lt;code class=&quot;highlighter-rouge&quot;&gt;(λx.x) λy.y*2 -&amp;gt; λy.y*2&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&quot;teorema-de-church-rosser&quot;&gt;Teorema de Church-Rosser&lt;/h3&gt;
&lt;blockquote&gt;
  &lt;p&gt;Establece que el orden en el que se apliquen estas reducciones no afecta al resultado final
* &lt;code class=&quot;highlighter-rouge&quot;&gt;(λx.x)(λy.y*2)3 -&amp;gt; (λy.y*2)3  -&amp;gt; 3*2&lt;/code&gt;
* &lt;code class=&quot;highlighter-rouge&quot;&gt;(λx.x)(λy.y*2)3 -&amp;gt; (λx.x)(3*2) -&amp;gt; 3*2&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;De modo que los paréntesis son usados normalmente para &lt;strong&gt;delimitar términos lambda&lt;/strong&gt;&lt;br /&gt;
Por ejemplo: &lt;code class=&quot;highlighter-rouge&quot;&gt;(λx.x)(λy.y)&lt;/code&gt;&lt;br /&gt;
Al evaluarse &lt;code class=&quot;highlighter-rouge&quot;&gt;(λy.y)&lt;/code&gt; se reduce, esto podría explicarse de manera muy coloquial: el resultado de la función se aplica como parámetro a otra función y realmente queda reducida a una sola.&lt;/p&gt;

&lt;p&gt;Sin embargo: &lt;code class=&quot;highlighter-rouge&quot;&gt;λx.xλy.y&lt;/code&gt; es una sola abstracción (función)&lt;/p&gt;

&lt;h3 id=&quot;variables-libres-y-ligadas&quot;&gt;Variables libres y ligadas&lt;/h3&gt;
&lt;p&gt;Sea una abstracción &lt;code class=&quot;highlighter-rouge&quot;&gt;λx.xy&lt;/code&gt; se dice que:
* &amp;gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;x&lt;/code&gt; esta &lt;strong&gt;ligada&lt;/strong&gt;
* &amp;gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;y&lt;/code&gt; es &lt;strong&gt;libre&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;En una &lt;strong&gt;&lt;em&gt;reducción-β&lt;/em&gt;&lt;/strong&gt; ó &lt;strong&gt;&lt;em&gt;sustitución&lt;/em&gt;&lt;/strong&gt; sólo se sustituyen las variables libres&lt;br /&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;(λx.x(λx.2+x)y)M&lt;/code&gt; &lt;strong&gt;→&lt;/strong&gt;  &lt;code class=&quot;highlighter-rouge&quot;&gt;x(λx.2+x)y[x:=M] = M(λx.2+x)y&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;El que sea la primera vez que lea sobre lambda cálculo y pueda leer esta línea sin llorar por dentro, bravo, pero normalmente y por experiencia propia, suele pasar que crees entender como lo hiciste y cuando se te da el problema sin la solución no estas tan seguro de lo que estas haciendo.&lt;/p&gt;

&lt;p&gt;[Añadir explicación paso a paso de la reducción]&lt;/p&gt;

&lt;p&gt;Para evitar estos conflictos en las reducciones se creó la &lt;strong&gt;conversión-α&lt;/strong&gt;&lt;br /&gt;
* Todas las apariciones de una variable &lt;strong&gt;ligada&lt;/strong&gt; en una misma abstracción se pueden renombrar a una &lt;strong&gt;nueva&lt;/strong&gt; variable
* Incapié en &lt;strong&gt;nueva&lt;/strong&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;funcional = variables inmutables&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;(λx.x(λx.2+x)y)M == (λx.x(λz.2+z)y)M → x(λz.2+z)y[x:=M] = M(λz.2+z)y &lt;/code&gt;&lt;br /&gt;
 y si deseas, lo puedes volver a renombrar &lt;code class=&quot;highlighter-rouge&quot;&gt;==  M(λx.2+x)y&lt;/code&gt;, ambas son soluciones válidas.&lt;/p&gt;

&lt;h3 id=&quot;conversin-&quot;&gt;conversión-α&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;λx.xy == λz.zy&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;λx.xy != λy.yy&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Nótese la diferencia, &lt;code class=&quot;highlighter-rouge&quot;&gt;y&lt;/code&gt; es una variable libre, no puede renombrarse:
&amp;gt; Todas las apariciones de una variable &lt;strong&gt;ligada&lt;/strong&gt; en una misma abstracción se pueden renombrar a una &lt;strong&gt;nueva&lt;/strong&gt; variable&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;λx.2+x == λy.2+y&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;xy(λx.2+x) == xy(λz.2+z)&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;…&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;isomorfismo-curry-howard&quot;&gt;Isomorfismo Curry-Howard&lt;/h2&gt;
&lt;p&gt;Este apartado lo voy a saltar por alto, lo que si voy a hacer es dejaros el enlace a la Wikipedia y resumir un poco brevemente de lo que trata.&lt;/p&gt;

&lt;p&gt;Básicamente debeís saber que al igual que en &lt;strong&gt;Ingeniería civil se fundamentan en el cálculo para hacer sus demostraciones&lt;/strong&gt;, en &lt;strong&gt;Ingeniería del Software nos fundamentamos en la lógica&lt;/strong&gt; para hacer inferencia y demostraciones.&lt;/p&gt;

&lt;p&gt;Por ejemplo:&lt;br /&gt;
&amp;gt; &lt;strong&gt;Verificación de programas&lt;/strong&gt;: demostrar que un programa es correcto conforme a una especificación:
* Un algoritmo de ordenación será correcto cuando se demuestre:&lt;br /&gt;
1. &lt;code class=&quot;highlighter-rouge&quot;&gt;Tras su invocación, todos los elementos están ordenados.&lt;/code&gt;
2. &lt;code class=&quot;highlighter-rouge&quot;&gt;Para cualquier entrada pasada al algoritmo (Infinitas).&lt;/code&gt;
&amp;gt; El isomorfismo o correspondencia de Curry-Howard establece una &lt;strong&gt;relación directa entre programas software y demostraciones matemáticas&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Para más información: &lt;a href=&quot;https://en.wikipedia.org/wiki/Curry%E2%80%93Howard_correspondence&quot;&gt;Wikipedia- Isomorfismo - Curry-Howard&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;funciones-entidades-primer-orden&quot;&gt;Funciones, Entidades Primer Orden&lt;/h2&gt;
&lt;blockquote&gt;
  &lt;p&gt;En &lt;strong&gt;paradigma funcional&lt;/strong&gt;, las funciones son entidades de primer orden, esto significa que las &lt;strong&gt;funciones son un tipo más&lt;/strong&gt;, pudiendose por ejemplo, instanciar variables de tipo función en estructuras de datos, pasarlas como parámetros o retornarlas.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;funciones-de-orden-superior&quot;&gt;Funciones de orden superior&lt;/h3&gt;
&lt;p&gt;Se dice que una función es de orden superior si:
* recibe alguna función como parámetro
* retorna alguna función como resultado
* Y por conclusión, si recibe y retorna, ¿lógico no?&lt;/p&gt;

&lt;p&gt;Por ejemplo, la función &lt;code class=&quot;highlighter-rouge&quot;&gt;doble&lt;/code&gt;: &lt;code class=&quot;highlighter-rouge&quot;&gt;λf.(λx.f(fx))&lt;/code&gt; sería una función de orden superior.&lt;/p&gt;

&lt;p&gt;En lenguajes funcionales puros como haskell, esto es nativo, todo son funciones, esto quiere decir que operadores como &lt;code class=&quot;highlighter-rouge&quot;&gt;+ - / ...&lt;/code&gt; son funciones infijas que curiosamente tambien pueden ser invocadas con &lt;a href=&quot;https://es.wikipedia.org/wiki/Notaci%C3%B3n_polaca_inversa&quot;&gt;notación polaca inversa&lt;/a&gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;+ 1 2&lt;/code&gt; = &lt;code class=&quot;highlighter-rouge&quot;&gt;3&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;El problema viene en lenguajes que siguen muy a raja tabla el paradigma orientado a objetos (Veáse Java…), como instancias funciones?, en Java 8 ya se han implementados cosas de funcional pero sigue siendo un completo desastre.&lt;/p&gt;

&lt;h3 id=&quot;delegados&quot;&gt;Delegados&lt;/h3&gt;
&lt;p&gt;C# usa algo llamado &lt;strong&gt;delegados&lt;/strong&gt;, los delegados son funciones que son &lt;strong&gt;entidades de primer orden&lt;/strong&gt;. (al igual que las lambda-expresiones).&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Un delegado constituye un &lt;strong&gt;tipo que representa un método&lt;/strong&gt; ya sea de instancia o de clase (&lt;code class=&quot;highlighter-rouge&quot;&gt;static&lt;/code&gt;)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Un poco de código para entender el concepto:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-C#&quot;&gt;public delegate int Comparacion(Persona p1, Persona p2);
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Comparación es un método que recibe dos &lt;code class=&quot;highlighter-rouge&quot;&gt;Persona&lt;/code&gt; y devuelve un &lt;code class=&quot;highlighter-rouge&quot;&gt;ìnt&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Ahora por ejemplo, podriamos definir un método &lt;code class=&quot;highlighter-rouge&quot;&gt;OrdenarPersonas&lt;/code&gt; independientemente del criterio de ordenación que se escoja.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-C#&quot;&gt;static public void OrdenarPersonas(Persona[] vector, Comparacion comparacion){
    //... Iterando el vector ...
    if(comparacion(vector[i], vector[j]) &amp;gt; 0){
        Persona aux = vector[i];
        vector[i] = vector[j];
        vector[j] = aux;
    }
    //...
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;tipos-de-delegados-predefinidos&quot;&gt;Tipos de delegados predefinidos&lt;/h3&gt;
&lt;blockquote&gt;
  &lt;p&gt;.Net tiene un conjunto de delegados predefinidos que hacen uso de la potencia de la &lt;strong&gt;genericidad&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Los más utilizados son &lt;code class=&quot;highlighter-rouge&quot;&gt;Func&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;Action&lt;/code&gt; y &lt;code class=&quot;highlighter-rouge&quot;&gt;Predicate&lt;/code&gt;.
* &amp;gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;Func&amp;lt;T&amp;gt;&lt;/code&gt;: Método sin parámetros que retorna un &lt;code class=&quot;highlighter-rouge&quot;&gt;T&lt;/code&gt;
* &amp;gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;Func&amp;lt;T1, T2&amp;gt;&lt;/code&gt;: Método que recibe un parámetro &lt;code class=&quot;highlighter-rouge&quot;&gt;T1&lt;/code&gt; y retorna un &lt;code class=&quot;highlighter-rouge&quot;&gt;T2&lt;/code&gt;
* &amp;gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;Action&lt;/code&gt;: Método sin parámetros ni retorno
* &amp;gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;Action&amp;lt;T&amp;gt;&lt;/code&gt;: Método con un parámetro &lt;code class=&quot;highlighter-rouge&quot;&gt;T&lt;/code&gt; sin retorno
* &amp;gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;Predicate&amp;lt;T&amp;gt;&lt;/code&gt;: Método que retorna un &lt;code class=&quot;highlighter-rouge&quot;&gt;bool&lt;/code&gt; y recibe un &lt;code class=&quot;highlighter-rouge&quot;&gt;T&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&quot;delegados-annimos&quot;&gt;Delegados Anónimos&lt;/h3&gt;
&lt;blockquote&gt;
  &lt;p&gt;En la programación funcional es común &lt;strong&gt;poder escribir la función en el momento de pasar ésta como parámetro&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&quot;language-C#&quot;&gt;Persona[] personas = ListadoPersonas.CrearPersonasAleatorias();
Persona[] mayoresEdad = Array.FindAll(personas, 
                            delegate(Persona p) { return p.Edad &amp;gt;= 18; }
                        );
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;C# lanzó una buena solución al problema pero la sintaxis seguia siendo dificil de tratar, y por eso decidio implementar &lt;strong&gt;expresiones lambda&lt;/strong&gt;, que básicamente mejoraban los &lt;strong&gt;delegados anónimos&lt;/strong&gt;&lt;/p&gt;

&lt;h3 id=&quot;expresiones-lambda-1&quot;&gt;Expresiones lambda&lt;/h3&gt;
&lt;blockquote&gt;
  &lt;p&gt;permiten &lt;strong&gt;escribir el cuerpo de funciones completas como expresiones&lt;/strong&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code class=&quot;language-C#&quot;&gt;//El ejemplo de función de orden superior que veiamos antes
Func&amp;lt;Func&amp;lt;int, int&amp;gt;, int, int&amp;gt; dobleAplicacion = (f, n) =&amp;gt; f(f(n));
Console.WriteLine(dobleAplicacion(n =&amp;gt; n+n, 3));

//Convertimos el ejemplo de los delegados anónimos
Persona[] personas = ListadoPersonas.CrearPersonasAleatorias();
Persona[] mayoresEdad = Array.FindAll(personas, persona =&amp;gt; persona.Edad &amp;gt;= 18);
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;En Haskell se utiliza esta sintaxis (Desde el intérprete):
&lt;code class=&quot;highlighter-rouge&quot;&gt;Haskell
-- DobleAplication de 3 -&amp;gt; 12
Prelude&amp;gt; (\f n -&amp;gt; f (f n)) (\x -&amp;gt; x+x) 3
12
&lt;/code&gt;
Haskell tiene inferencia de tipos, pero tiene la característica de poder definir una cabecera con los tipos
&lt;code class=&quot;highlighter-rouge&quot;&gt;Haskell
factorial :: Integer -&amp;gt; Integer
factorial n = product [1..n]
&lt;/code&gt;&lt;/p&gt;

&lt;h3 id=&quot;bucles-y-recursividad&quot;&gt;Bucles y Recursividad&lt;/h3&gt;
&lt;p&gt;Me imagino que ya os habreís pegado con el concepto de recursividad más veces, y en parte, como &lt;strong&gt;C# es un lenguaje funcional no puro, puede usar estructuras de control de flujo iterativas cuando sea oportuno&lt;/strong&gt;, pero los &lt;strong&gt;lenguajes puros como Haskell hacen uso exhaustivo de la recursión&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Por ejemplo, imaginate que los operadores: &lt;code class=&quot;highlighter-rouge&quot;&gt;if&lt;/code&gt;-&lt;code class=&quot;highlighter-rouge&quot;&gt;then&lt;/code&gt;-&lt;code class=&quot;highlighter-rouge&quot;&gt;else&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;=&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;-&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;*&lt;/code&gt;los tenemos codificados en lambda cálculo y podemos usarlos.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Función factorial en lambda cálculo:
&lt;code class=&quot;highlighter-rouge&quot;&gt;λf.λx. if x=0 or x=1 then 1 else x*f(x-1)&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Hay una función de orden superior muy conocida en haskell: &lt;code class=&quot;highlighter-rouge&quot;&gt;fix&lt;/code&gt; ó &lt;strong&gt;combinador de punto fijo&lt;/strong&gt; que suele ser muy utilizada y que podremos aplicar tambien en lambda cálculo si estuviera definida previamente.&lt;br /&gt;
&amp;gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;fix f -&amp;gt; f (fix f)&lt;/code&gt;
* al aplicar &lt;code class=&quot;highlighter-rouge&quot;&gt;fix&lt;/code&gt; a &lt;code class=&quot;highlighter-rouge&quot;&gt;f (fix f)&lt;/code&gt;
    1. Se retorna f&lt;br /&gt;
    2. Pasando una nueva invocación a &lt;code class=&quot;highlighter-rouge&quot;&gt;fix f&lt;/code&gt; como primer parámetro&lt;/p&gt;

&lt;h2 id=&quot;ejercicios-para-prcticar-lo-aprendido&quot;&gt;Ejercicios para prácticar lo aprendido&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Aplica reducción y conversión a: &lt;code class=&quot;highlighter-rouge&quot;&gt;(λf.λx.f(fx))(λx.x+x)n&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Usando funciones de orden superior, preferiblemente, usando lambda expresiones, construye tu propia “Calculadora funcional” con las operaciones básicas de : &lt;code class=&quot;highlighter-rouge&quot;&gt;suma&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;resta&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;multiplicación&lt;/code&gt; y &lt;code class=&quot;highlighter-rouge&quot;&gt;divisiòn&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;soluciones&quot;&gt;Soluciones&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Lambda cálculo - solución: &lt;a href=&quot;https://github.com/Indeseables/indeseables.github.io/blob/master/_codigos/entrada_funcional1/555df00550955abc13e4cec1400672c7.png&quot;&gt;Github: Imágen&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Calculadora funcional: &lt;a href=&quot;https://github.com/Indeseables/indeseables.github.io/blob/master/_codigos/entrada_funcional1/calculator.hs&quot;&gt;Github: Haskell&lt;/a&gt; &lt;a href=&quot;https://github.com/Indeseables/indeseables.github.io/blob/master/_codigos/entrada_funcional1/calculator.cs&quot;&gt;Github: C#&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;siguiente-post&quot;&gt;Siguiente post&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Cláusulas, Currificación y Aplicación Parcial&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;referencias&quot;&gt;Referencias&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Códigos de ejemplo y Teoria casi en su totalidad extraidos de las diapositivas de la Escuela de Ingeneria Informática de Uniovi&lt;/strong&gt;, &lt;strong&gt;autor&lt;/strong&gt; según indícan las diapositivas: &lt;strong&gt;Francisco Ortín Soler&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;Famoso libro de haskell : &lt;a href=&quot;http://aprendehaskell.es&quot;&gt;Aprende Haskell&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;autora&quot;&gt;Autoría&lt;/h2&gt;
&lt;p&gt;Apuntes de Tecnología y Paradigmas de la programación escritos por: &lt;strong&gt;Esteban Montes Morales&lt;/strong&gt;
* Perfil de Github: &lt;a href=&quot;http://github.com/sankosk&quot;&gt;Click para ir al sitio&lt;/a&gt;
* Twitter: @SankoSK
* Perfil de Linkedin: &lt;a href=&quot;https://www.linkedin.com/in/esteban-montes-morales-59257366&quot;&gt;Click para ir al sitio&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Si tienes alguna duda, comentario o sugerencia, notifícanos a través de &lt;a href=&quot;mailto:indeseables.git@gmail.com&quot;&gt;indeseables.git@gmail.com&lt;/a&gt;
 y si te ha gustado la entrada puedes &lt;a href=&quot;https://twitter.com/intent/tweet?url=http://indeseables.github.io/2016/03/28/odisea-funcional-parte1/&amp;amp;text=Odisea Funcional (Parte 1)&amp;amp;via=indeseables!&quot; target=&quot;_blank&quot;&gt; compartirla!.&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;
</content>
 </entry>
 
 <entry>
   <title>Redes neuronales y autoencoders.</title>
   <link href="http://indeseables.github.io/2016/03/27/Redes-neuronales-y-Autoencoders/"/>
   <updated>2016-03-27T00:00:00+01:00</updated>
   <id>http://indeseables.github.io/2016/03/27/Redes neuronales y Autoencoders</id>
   <content type="html">&lt;p&gt;Si se ha visto el &lt;a href=&quot;http://indeseables.github.io//2016/03/25/implementacion-alternativa-algoritmos-aprendizaje/&quot;&gt;post&lt;/a&gt; anterior, en el que se comentó la idea general del funcionamiento del Perceptron y se propuso una implementación usando Scipy, la definición de red neuronal como un conjunto de procesadores elementales (neuronas) densamente interconectados no nos resultará difícil de entender, ya que no son más que un conjunto de &lt;a href=&quot;https://i.gyazo.com/66efec53d6c5232254e070ba0256c949.png&quot;&gt;Perceptron dispuestos en cascada&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;redes-neuronales&quot;&gt;Redes neuronales&lt;/h3&gt;
&lt;p&gt;También llamadas redes neuronales artificiales o procesado distribuido y paralelo, se basan en el mismo principio que Perceptron, funciones discriminantes lineales (&lt;a href=&quot;https://i.gyazo.com/f895cd0241ad3206721e1444907b4b02.png&quot;&gt;FDL&lt;/a&gt;) que se determinan mediante el aprendizaje de un vector de parámetros &lt;em&gt;θ&lt;/em&gt; resolviendo un problema de optimización en el que se pretende obtener el vector &lt;em&gt;θ&lt;/em&gt; que minimice el error cuadrático entre la salida esperada de las muestras de entrenamiento y la salida obtenida por la red neuronal.Este vector de parámetros &lt;em&gt;θ&lt;/em&gt;, hace referencia al peso de las conexiones entre las neuronas de la red neuronal (los pesos asociados a cada una de las aristas de la red de la &lt;a href=&quot;https://i.gyazo.com/66efec53d6c5232254e070ba0256c949.png&quot;&gt;imagen&lt;/a&gt;) y es una medida de la complejidad de la red.&lt;/p&gt;

&lt;p&gt;Podemos distinguir 2 aspectos de dichas redes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Topología: hace referencia a la &lt;a href=&quot;https://i.gyazo.com/66efec53d6c5232254e070ba0256c949.png&quot;&gt;estructura de la red&lt;/a&gt;. Definimos una capa como un conjunto de neuronas en un mismo nivel (no hay conexiones entre ellas) y llamaremos a la primera capa (la de las muestras &lt;em&gt;{x1,x2,…,xn}&lt;/em&gt;, capa de salida a la última capa y capas ocultas a todas las capas intermedias.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Dinámica: describe como fluye la información a través de la red neuronal p.e. &lt;a href=&quot;https://i.gyazo.com/3a8b0706c7ce5dfb25cebda4edf6ad11.png&quot;&gt;para el caso de la topología anterior&lt;/a&gt;. Se puede observar como, para cada neurona &lt;em&gt;i&lt;/em&gt; de cada capa (oculta y de salida), se calcula su salida en función de los pesos de las aristas que alcanzan a &lt;em&gt;i&lt;/em&gt; y del valor de las neuronas &lt;em&gt;j&lt;/em&gt; de la capa anterior que alcanzan a &lt;em&gt;i&lt;/em&gt; a través de &lt;em&gt;θij&lt;/em&gt; aplicando una función &lt;em&gt;g&lt;/em&gt; al resultado obtenido (este tipo de redes neuronales se llaman feed-forward, en las solo hay conexiones hacia delante, en concreto, el tipo de perceptron multicapa que comentamos es un caso particular de red feed-forward donde solo hay conexiones a neuronas de la capa siguiente) algunos ejemplos de funciones de este tipo son &lt;a href=&quot;https://i.gyazo.com/24422f154964f9b4f08ecd6ca45181a6.png&quot;&gt;estos&lt;/a&gt;. Hay toda una historia asociada a éstas funciones, pero sólo considerad que éstas funciones se utilizan para que la red neuronal pueda aprender fronteras de decisión más complejas.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Las redes neuronales pueden aplicarse tanto a regresión (para lo que fueron concebidas) como a clasificación, donde cualquier frontera de decisión basada en trozos de hiperplanos se puede aproximar con un perceptron multicapa de este estilo. Además, pueden ser empleadas tanto en &lt;a href=&quot;https://es.wikipedia.org/wiki/Aprendizaje_supervisado&quot;&gt;aprendizaje supervisado&lt;/a&gt;, como en &lt;a href=&quot;https://es.wikipedia.org/wiki/Aprendizaje_no_supervisado&quot;&gt;aprendizaje no supervisado&lt;/a&gt; (e.g. autoencoders que veremos en el próximo apartado)&lt;/p&gt;

&lt;p&gt;Aparte de lo que he comentado, hay muchísimos teoremas relacionados con las redes neuronales, relativos a la convergencia, al factor de aprendizaje, la intratabilidad del aprendizaje (Blum and Rivest, 1992), el tamaño del conjunto de entrenamiento (Ripley, 1993) y muchos otros que no comentaré, además, a los que estén familiarizados con ésto tal vez les resulte raro que no comente el tan conocido algoritmo de aprendizaje &lt;a href=&quot;https://es.wikipedia.org/wiki/Propagaci%C3%B3n_hacia_atr%C3%A1s&quot;&gt;&lt;em&gt;backpropagation&lt;/em&gt;&lt;/a&gt;, pero me lo ahorraré ya que no deja de ser un método de optimización de descenso por gradiente mediante las funciones de activación &lt;em&gt;g&lt;/em&gt;, comentadas antes, que simplifican el cálculo de las derivadas parciales. Si no estáis familiarizados, este proceso vedlo como una caja negra que calcula los pesos óptimos de las conexiones entre las neuronas de la red, para la entrada, lo implementaré con los optimizadores de Scipy (y podemos aplicar métodos no convencionales de optimización para calcular el vector &lt;em&gt;θ&lt;/em&gt;, aparte del descenso por gradiente! ). Para implementarlo, basta considerar que el aprendizaje consiste en minimizar el error cuadrático medio de las diferencias entre las salidas esperadas y las obtenidas, e.g. &lt;a href=&quot;https://i.gyazo.com/dc04d1204eabedaffd8f62b62fbf1962.png&quot;&gt;1 capa oculta&lt;/a&gt;, por tanto, podemos considerar dicha función como función objetivo y calcular &lt;em&gt;θ&lt;/em&gt; óptimo mediante Scipy, con cualquiera de los algoritmos de optimización implementados.&lt;/p&gt;

&lt;p&gt;Con ello, ya sé ha visto una pincelada (muy gruesa) de lo básico para comprender mínimamente la implementación de un ejemplo que permite definir una red neuronal y dado un conjunto de muestras supervisadas (o no supervisadas), aprender el vector de parámetros &lt;em&gt;θ&lt;/em&gt; para clasificar o hacer regresión.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;#!/usr/bin/env python&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# -*- coding: utf-8 -*-&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#  NeuralNetwork.py&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;scipy.optimize&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;minimize&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;Utils&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MatLoad&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;math&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;lineal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;ramp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;elif&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;elif&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;=-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;	
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;sigmoid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;tanh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;fast&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;abs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;get_weights_submatrices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;units_by_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;thetas&lt;/span&gt;    &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;     &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;units_by_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;units_by_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;thetas&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;units_by_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;units_by_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;xrange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;units_by_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;thetas&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;units_by_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;units_by_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))])&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;units_by_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;units_by_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;index&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;units_by_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;units_by_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;thetas&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:])&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;units_by_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;units_by_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;units_by_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;units_by_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;thetas&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;generate_theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;units_by_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;n_connections&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;xrange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;units_by_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_connections&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;units_by_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;units_by_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;n_connections&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;units_by_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;units_by_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;units_by_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;units_by_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;xrange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_connections&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)])&lt;/span&gt;
    
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward_propagation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;units_by_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;factivation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;               &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_weights_submatrices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;units_by_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;vec_factivation&lt;/span&gt;     &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vectorize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;factivation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;res&lt;/span&gt;                 &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vec_factivation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;xrange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;units_by_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;res&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;insert&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;res&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;xrange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
	&lt;span class=&quot;n&quot;&gt;res&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vec_factivation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;res&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;res&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward_propagation_transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;units_by_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;factivation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;vec_factivation&lt;/span&gt;     &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vectorize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;factivation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;vec_factivation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;units_by_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;factivation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;res_output_layer&lt;/span&gt;    &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;forward_propagation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;units_by_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;factivation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;mean_squared_error&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;res_output_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;mean_squared_error&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;2.0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;multiply&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean_squared_error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean_squared_error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mean_squared_error&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;units_by_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;factivation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;max_iter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;verbose_convergence&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;minimize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fun&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;units_by_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;factivation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;method&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;CG&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;options&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;maxiter&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;max_iter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;disp&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;verbose_convergence&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;classify&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;units_by_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;factivation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;forward_propagation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;units_by_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;factivation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;argmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;units_by_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;factivation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;forward_propagation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;units_by_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;factivation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
 
&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__name__&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;__main__&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    
    &lt;span class=&quot;c&quot;&gt;## Testing classifier ##&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MatLoad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;X.np&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# Vectores por columnas (N=|cols|, M=|rows|) # &lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MatLoad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Y.np&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# Vectores por columnas (N=|cols|, M=|rows|) #&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;XROWS&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;XCOLS&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    
    &lt;span class=&quot;c&quot;&gt;#### Test NN aleatoria ####&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;units_by_layer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;XROWS&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Se cuenta la unidad BIAS en la capa oculta (la entrada se asume homogénea) en la de salida NO hay #&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;generate_theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;units_by_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;res&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;units_by_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lineal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; Detalles de convergencia &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;res&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;res&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Clase de la muestra [1,-4,-4]: &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;classify&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;units_by_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lineal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Clase de la muestra [1,4,4]: &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;classify&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;units_by_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lineal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Regresion con la muestra [1,-4,-4]: &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;units_by_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lineal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Regresion con la muestra [1,-4,-4]: &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;units_by_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lineal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;En el ejemplo se puede ver como se carga el fichero que tiene las muestras &lt;em&gt;X&lt;/em&gt; y el que tiene las salidas de dichas muestras &lt;em&gt;Y&lt;/em&gt; (la salida de cada muestra, ahora es un conjunto de valores que representan el valor esperado en cada una de las neuronas de salida). Después se define una red neuronal especificando el número de neuronas por capa, en este caso, una red con 4 capas (las de entrada y salida y 2 capas ocultas), donde:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;La capa 0, de entrada tiene tantas neuronas como dimensiones tienen las muestras.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;La capa 1, (1º capa oculta) tiene 4 neuronas (en el código son 5 porque a las capas ocultas hay que sumarle 1, debido a la neurona de bias (&lt;em&gt;+1&lt;/em&gt;) que representa el término independiente en la combinación lineal que define la FLD (en la capa de entrada, la unidad de bias viene representada en las muestras al ponerlas en notación homogénea - poniendo un 1 en la primera posición - y en la capa de salida no hay bias).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;La capa 2 (2º capa oculta) tiene 3 neuronas (en el código 4 por la razón comentada antes).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;La capa 3, capa de salida tiene tantas neuronas como dimensiones tiene el conjunto de salida esperado (clases en clasificación o nº dimensiones en regresión).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;La salida del script con &lt;a href=&quot;https://i.gyazo.com/12a7773066427859ab109f9c0b2b9e2b.png&quot;&gt;X&lt;/a&gt;, &lt;a href=&quot;https://i.gyazo.com/f01eb26b0965ea40a8eda8a3250a2b29.png&quot;&gt;Y&lt;/a&gt; (muestras y salidas dispuestas por columnas) es la siguiente:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
Optimization terminated successfully.
         Current function value: 0.075000
         Iterations: 41
         Function evaluations: 2849
         Gradient evaluations: 77

 Detalles de convergencia

     fun: 0.07500000000044556
     jac: array([ -1.40629709e-07,  -2.80328095e-07,  -2.80328095e-07,
        -1.42492354e-07,  -2.79396772e-07,  -2.79396772e-07,
        -1.40629709e-07,  -2.80328095e-07,  -2.80328095e-07,
        -1.41561031e-07,  -2.80328095e-07,  -2.80328095e-07,
         5.92321157e-07,  -5.71832061e-07,  -5.71832061e-07,
        -5.71832061e-07,  -5.72763383e-07,   5.92321157e-07,
        -5.72763383e-07,  -5.72763383e-07,  -5.72763383e-07,
        -5.71832061e-07,   5.92321157e-07,  -5.71832061e-07,
        -5.71832061e-07,  -5.71832061e-07,  -5.72763383e-07,
        -4.53554094e-07,  -3.41795385e-07,  -3.43658030e-07,
        -3.41795385e-07,   7.41332769e-07,   5.39235771e-07,
         5.38304448e-07,   5.39235771e-07])
 message: &#39;Optimization terminated successfully.&#39;
    nfev: 2849
     nit: 41
    njev: 77
  status: 0
 success: True
       x: array([ 0.29015638, -0.31882134, -0.31882134,  0.29015639, -0.31882129,
       -0.31882129,  0.29015638, -0.31882134, -0.31882134,  0.29015634,
       -0.31882134, -0.31882134,  0.42504066, -0.07970499, -0.07970504,
       -0.07970499, -0.07970508,  0.42504011, -0.07970658, -0.07970659,
       -0.07970658, -0.07970657,  0.42504066, -0.07970499, -0.07970504,
       -0.07970499, -0.07970508,  1.19071489, -0.49189776, -0.49189632,
       -0.49189776, -0.19071479,  0.49189641,  0.49189929,  0.49189641])
Clase de la muestra [1,-4,-4]:  0
Clase de la muestra [1,4,4]:  1
Regresion con la muestra [1,-4,-4]:  [[ 1.90000067]
 [-0.90000071]]
Regresion con la muestra [1,-4,-4]:  [[-0.50000098]
 [ 1.50000139]]
 
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Se puede ver que para clasificación (índice de la neurona de la capa de salida que maximice su salida) con esas muestras se comporta bien (dibujadlo en un eje cartesiano teniendo en cuenta la salida de cada muestra e intentad separarlas) y mediante regresión podemos conocer el valor de cada una de las neuronas de la capa de salida. En este ejemplo se ha visto como podemos entrenar de forma supervisada una red neuronal, sin embargo pueden ser entrenadas de forma no supervisada de una manera muy peculiar.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;autoencoders&quot;&gt;Autoencoders&lt;/h3&gt;
&lt;p&gt;Un &lt;a href=&quot;https://en.wikipedia.org/wiki/Autoencoder&quot;&gt;autoencoder&lt;/a&gt; es una red neuronal utilizada para aprender codificaciones (p.e. &lt;a href=&quot;https://es.wikipedia.org/wiki/Reducci%C3%B3n_de_dimensionalidad&quot;&gt;reducción de dimensionalidad&lt;/a&gt;, compresión, cifrado, etc) eficientes. Comentaré el autoencoder más básico, que es un caso particular de redes feed-forward muy parecido al perceptron multicapa comentado antes, en el que la capa de entrada y la capa de salida tienen el mismo número de neuronas y en lugar de esperar una salida supervisada &lt;em&gt;Y&lt;/em&gt;, se espera la misma muestra de entrada &lt;em&gt;X&lt;/em&gt;, consiguiendo con ello que la red aprenda a reconstruir la misma entrada que recibe.&lt;/p&gt;

&lt;p&gt;La estructura general de un autoencoder es &lt;a href=&quot;https://upload.wikimedia.org/wikipedia/commons/2/28/Autoencoder_structure.png&quot;&gt;ésta&lt;/a&gt;, y el que emplearé en esta entrada es una variación de &lt;a href=&quot;https://rubenlopezg.files.wordpress.com/2014/04/sparse-autoencoder2.png&quot;&gt;éste&lt;/a&gt; en el que se ha modificado el número de neuronas en cada capa (ajustándolo a nuestras muestras de entrenamiento y a lo que se quiera conseguir).&lt;/p&gt;

&lt;p&gt;Si nos fijamos, cuando se entrene la red neuronal de la &lt;a href=&quot;https://rubenlopezg.files.wordpress.com/2014/04/sparse-autoencoder2.png&quot;&gt;figura del autoencoder anterior&lt;/a&gt; se calcularán los pesos de todas las conexiones de la red y con ello, podemos separar la red en 2 subredes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Subred 1: formada por la capa de entrada y la capa oculta, el resultado obtenido de esta subred es el valor de las neuronas que forman la capa oculta, donde se habrá obtenido una transformación de las muestras reales a un espacio alternativo. Por esta razón, a esta subred se le llama &lt;em&gt;encoder&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Subred 2: formada por la capa oculta y la capa de salida, en este caso, partiendo del resultado obtenido en las neuronas de la capa oculta en el paso anterior (resultado del encoder) se obtiene el valor de las neuronas de la capa de salida, que recordemos, sus pesos habían sido entrenados (junto con los demás de la red) para reconstruir la entrada de salida. Con ello, podemos obtener a partir del resultado del &lt;em&gt;encoder&lt;/em&gt; el resultado original. Por esta razón, a esta subred se le llama &lt;em&gt;decoder&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Por tanto, como se ha visto, al ser el &lt;em&gt;autoencoder&lt;/em&gt; simple un caso particular de red neuronal, podemos hacer uso del script implementado en el apartado anterior, que nos permite diseñar redes neuronales y aprender los pesos que minimicen el error cuadrático medio. Consideraré 2 ejemplos de uso de estos &lt;em&gt;autoencoder&lt;/em&gt;, el cifrado de bloques de texto y la compresión.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;aplicaciones-de-autoencoders-sistema-de-cifrado-por-bloques-1&quot;&gt;Aplicaciones de Autoencoders: sistema de cifrado por bloques (1)&lt;/h3&gt;

&lt;p&gt;En el primer ejemplo de aplicación, vamos a ver como haciendo uso de un &lt;em&gt;autoencoder&lt;/em&gt; como los explicados arriba, podemos construir un &lt;a href=&quot;https://es.wikipedia.org/wiki/Cifrado_por_bloques&quot;&gt;sistema de cifrado por bloques&lt;/a&gt; muy simple, en el que el &lt;em&gt;encoder&lt;/em&gt; nos permitirá realizar el proceso de cifrado y el &lt;em&gt;decoder&lt;/em&gt; el proceso de descifrado. Supongamos que queremos cifrar el mensaje &lt;em&gt;“holablau”&lt;/em&gt;, podemos por ejemplo construir 4 muestras de entrenamiento segmentando el mensaje en conjuntos de 2B en secuencia i.e. &lt;em&gt;S={“ho”,”la”,”bl”,”au”}&lt;/em&gt; (tendremos que cifrar y descifrar bloques de 2B) , como hay que obtener una representación en un espacio continuo, podemos obtener para cada elemento de &lt;em&gt;S&lt;/em&gt; el valor binario de su código &lt;em&gt;ASCII&lt;/em&gt; (concatenar los bits de cada uno de los símbolos de cada elemento), con ello, disponemos cada muestra de entrenamiento en columnas (y en notación homogénea con el 1 delante!) y no necesitamos más (recordad que no se requiere información de la salida, solo se pretende reconstruir la muestra original).&lt;/p&gt;

&lt;p&gt;Ahora solo queda fijar el número de neuronas en la capa oculta (&lt;em&gt;M1&lt;/em&gt; en el script a 10 , 9 neuronas y la de BIAS) y una vez establecido, ejecutar el script (implementado haciendo uso del script para redes neuronales -MLP- del apartado anterior -NeuralNetwork.py-):&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;c&quot;&gt;#!/usr/bin/env python&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# -*- coding: utf-8 -*-&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#  AutoEncoder.py&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;NeuralNetwork&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;nn&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;Utils&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MatLoad&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__name__&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;__main__&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MatLoad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;X.np&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# Vectores por columnas (N=|cols|, M=|rows|) # &lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;XROWS&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;XCOLS&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;M1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Se cuenta la unidad BIAS en la capa oculta (la entrada se asume en notación homogénea) #&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;units_by_layer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;XROWS&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;M1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;XROWS&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;generate_theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;units_by_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;     &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;res&lt;/span&gt;   &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;units_by_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lineal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;max_iter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1000000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; Detalles de convergencia &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;res&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;res&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_weights_submatrices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;units_by_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Vector original: &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;encoded&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;forward_propagation_transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;units_by_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lineal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;encoded&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;insert&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoded&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Vector cifrado: &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoded&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;decoded&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;forward_propagation_transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoded&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;units_by_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lineal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;Vector descifrado: &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decoded&lt;/span&gt;


&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;En el script se ha entrenado primero el &lt;em&gt;autoencoder&lt;/em&gt; con el conjunto de muestras que he mencionado antes y después se ha procedido a cifrar el bloque de 2B “ho” (&lt;code class=&quot;highlighter-rouge&quot;&gt;0110100001101111&lt;/code&gt;, y en notación homogénea &lt;code class=&quot;highlighter-rouge&quot;&gt;10110100001101111&lt;/code&gt;), los resultados que se obtienen tras la ejecución del script son:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
Optimization terminated successfully.
         Current function value: 0.000000
         Iterations: 134
         Function evaluations: 105841
         Gradient evaluations: 336

 Detalles de convergencia

     fun: 2.399946487631182e-09
     jac: array([ -1.10356042e-06,   0.00000000e+00,  -1.10356042e-06,
     ...
      message: &#39;Optimization terminated successfully.&#39;
    nfev: 105841
     nit: 134
    njev: 336
  status: 0
 success: True
       x: array([ -8.47539898e-01,   1.00000000e+00,  -8.47539898e-01,
       ...
       Vector cifrado:  
[[ 1.        ]
[-5.18020183]
[-5.18122276]
[-5.54306646]
[-4.99780952]
[-4.88438915]
[-5.32546369]
[-4.55401426]
[-5.17546803]
[-6.14500501]]
    Vector descifrado:  
[[  2.73595684e-05]
 [  9.99997815e-01]
 [  9.99997815e-01]
 [  2.73595684e-05]
 [  1.00001467e+00]
 [  2.51373514e-05]
 [  1.14924639e-05]
 [  2.63763669e-05]
 [  2.73599263e-05]
 [  9.99997815e-01]
 [  9.99997815e-01]
 [  2.63777148e-05]
 [  1.00000102e+00]
 [  1.00000004e+00]
 [  1.00001689e+00]
 [  1.00001368e+00]]
 
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Se puede observar como los 2 bytes &lt;em&gt;“ho”&lt;/em&gt;, cifrados mediante el &lt;em&gt;encoder&lt;/em&gt;, quedan de la siguiente forma:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
[1.0, -5.1802018305848385, -5.181222759796455, -5.543066463152387, -4.997809523535919, -4.884389148007498, -5.325463692424654, -4.554014256199364, -5.1754680337452275, -6.145005007361612]

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Podemos transmitir ese cifrado a través de un canal inseguro, habiendo comunicado previamente la clave, que en este caso son los pesos del &lt;em&gt;decoder&lt;/em&gt; y el número de neuronas en la capa de salida, mediante un canal seguro p.e. obtenido con criptografía de clave pública.En el destino se descifra mediante el &lt;em&gt;decoder&lt;/em&gt; partiendo del cifrado anterior, y se obtiene:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
[2.73595684e-05, 0.999997815, 0.999997815, 2.73595684e-05, 1.00001467, 2.51373514e-05, 1.14924639e-05, 2.63763669e-05, 2.73599263e-05, 0.999997815, 0.999997815,2.63777148e-05, 1.00000102, 1.00000004, 1.00001689, 1.00001368]
 
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Si son valores absolutos p.e. 0 y 1, posiblemente no se lleguen a alcanzar nunca debido a que la máxima precisión en coma flotante son 16 dígitos correctos, sin embargo se obtienen valores que prácticamente se corresponden con los valores extremos. En este caso: &lt;code class=&quot;highlighter-rouge&quot;&gt;[0,1,1,0,1,0,0,0,0,1,1,0,1,1,1,1]&lt;/code&gt; que se corresponde con el bloque de 2B original: &lt;code class=&quot;highlighter-rouge&quot;&gt;0110100001101111&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;De éste modo, podríamos cifrar mensajes completos segmentándolos en bloques de 2B consecutivos e ir cifrando bloque a bloque mediante modos de operación (encadenados o no). Pero si nos fijamos, no solo hemos cifrado, si no que también hemos reducido la información a transmitir (aunque no del todo porque en este ejemplo, al ser la representación discreta y binaria se podría enviar como bloques de bits concatenados y al cifrar de esta manera obtenemos un conjunto de reales que ocupan un mayor número de bytes) pasando de 16 elementos (2B por bloque) a 9 elementos. ¿por qué ha pasado ésto?-&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;aplicaciones-de-autoencoders-compresin-2&quot;&gt;Aplicaciones de Autoencoders: compresión (2)&lt;/h3&gt;
&lt;p&gt;En el ejemplo anterior se ha visto como se ha reducido el número de elementos a transmitir, ¿por qué?, la respuesta es sencilla, si partimos de una muestra con 16 dimensiones y la usamos para obtener una salida en la primera capa oculta(la última del &lt;em&gt;encoder&lt;/em&gt;) de la red neuronal entrenada para la reconstrucción de las muestras, si esta primera capa oculta tiene un menor número de neuronas que dimensiones tienen las muestras, estaremos consiguiendo una reducción del número de elementos. En el ejemplo del cifrado se puede ver como cada muestra tiene 16 dimensiones (1 por cada bit del bloque de 2B) y la primera capa oculta tiene 9 neuronas, por lo que pasaremos de tener un bloque de 16 bits del texto sin cifrar a obtener 9 elementos reales de texto cifrado (se ha obtenido una reducción del número de elementos en el espacio original).&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Con esto ha quedado una entrada que recoge una pequeña parte de la teoría de redes neuronales y propone una implementación resolviendo el problema de optimización que caracteriza el aprendizaje en este tipo de redes con métodos diferentes a los convencionales (todos los implementados en el módulo &lt;em&gt;optimize&lt;/em&gt; de &lt;em&gt;Scipy&lt;/em&gt; de ámbito local e.g. gradiente conjugado, L-BFGS, L-BFGS-B, Nelder-Mead etc).&lt;/p&gt;

&lt;p&gt;En la siguiente entrada continuaré las entregas de implementaciones alternativas explicando SVM y funciones kernel.&lt;/p&gt;

&lt;p&gt;He dejado todo el código nuevo en el &lt;a href=&quot;https://github.com/Indeseables/indeseables.github.io/tree/master/_codigos/Entrada_2&quot;&gt;repositorio&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Cualquier comentario:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Email: indeseables [at] gmail [dot] com&lt;/li&gt;
  &lt;li&gt;Twitter: &lt;a href=&quot;http://twitter.com/&quot;&gt;@indeseables&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Issues: &lt;a href=&quot;https://github.com/Indeseables/indeseables.github.io/issues&quot;&gt;#Issues&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;Si tienes alguna duda, comentario o sugerencia, notifícanos a través de &lt;a href=&quot;mailto:indeseables.git@gmail.com&quot;&gt;indeseables.git@gmail.com&lt;/a&gt;
 y si te ha gustado la entrada puedes &lt;a href=&quot;https://twitter.com/intent/tweet?url=http://indeseables.github.io/2016/03/27/Redes-neuronales-y-Autoencoders/&amp;amp;text=Redes neuronales y autoencoders.&amp;amp;via=indeseables!&quot; target=&quot;_blank&quot;&gt; compartirla!.&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;
</content>
 </entry>
 
 <entry>
   <title>Implementación alternativa de algoritmos de aprendizaje (parte I).</title>
   <link href="http://indeseables.github.io/2016/03/25/implementacion-alternativa-algoritmos-aprendizaje/"/>
   <updated>2016-03-25T00:00:00+01:00</updated>
   <id>http://indeseables.github.io/2016/03/25/implementacion-alternativa-algoritmos-aprendizaje</id>
   <content type="html">&lt;p&gt;Estrenaré el blog con el inicio de una serie de entradas que dedicaré a implementar algoritmos de aprendizaje usando la librería &lt;em&gt;Scipy&lt;/em&gt; y más concretamente, su módulo &lt;em&gt;optimize&lt;/em&gt; (tenía pensado usar &lt;a href=&quot;https://www.mcs.anl.gov/petsc/&quot;&gt;Petsc&lt;/a&gt;, pero no he conseguido convertir matrices a su formato y me ha complicado las cosas).&lt;/p&gt;

&lt;p&gt;Dicho módulo nos permite realizar varios métodos de optimización y búsqueda de raices de funciones (nos centraremos en la parte de optimización de momento y tal vez vea optimizaciones alternativas p.e. usando &lt;a href=&quot;https://es.wikipedia.org/wiki/M%C3%A9todo_iterativo&quot;&gt;métodos iterativos&lt;/a&gt; de &lt;a href=&quot;https://es.wikipedia.org/wiki/M%C3%A9todo_iterativo#M.C3.A9todos_del_subespacio_de_Krylov&quot;&gt;Krylov&lt;/a&gt;, que son de mucha utilidad a la hora de resolver sistemas muy grandes de forma eficiente) como pueden ser (no comentaré todos los &lt;a href=&quot;http://docs.scipy.org/doc/scipy/reference/optimize.html&quot;&gt;métodos&lt;/a&gt; de cada tipo de optimización):&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Optimización local: permite minimizar (y si cambiamos de signo maximizar) &lt;a href=&quot;https://es.wikipedia.org/wiki/Optimizaci%C3%B3n_%28matem%C3%A1tica%29&quot;&gt;funciones&lt;/a&gt;, variando una serie de parámetros del optimizador como el método de optimización local empleado (Gradiente conjugado, BFGS, Método de Powell …). Al ser un &lt;a href=&quot;(https://en.wikipedia.org/wiki/Local_search_%28optimization%29)&quot;&gt;método local&lt;/a&gt; , puede quedar atascado en mínimos locales por lo que, si la solución obtenida no es suficientemente buena, se pueden obtener nuevas soluciones de varias formas p e. reinicialización aleatoria, son computacionalmente baratos.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Optimización de ecuaciones: entre los que se puede encontrar el método de &lt;a href=&quot;https://es.wikipedia.org/wiki/M%C3%ADnimos_cuadrados&quot;&gt;mínimos cuadrados&lt;/a&gt; ampliamente conocidos en &lt;a href=&quot;https://es.wikipedia.org/wiki/Regresi%C3%B3n_%28estad%C3%ADstica%29&quot;&gt;regresión&lt;/a&gt; y optimización&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Optimizaciones globales: al contrario que los métodos locales, éstos nos proporcionan soluciones óptimas pero son más costosos ya que requieren comprobar el espacio de soluciones completo (a no ser que se disponga de información que lo acote).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Ajustes de funciones (&lt;em&gt;fitting&lt;/em&gt;): dada una función &lt;em&gt;f&lt;/em&gt;, encontrar una función &lt;em&gt;g&lt;/em&gt; que la &lt;a href=&quot;https://www.google.es/url?sa=t&amp;amp;rct=j&amp;amp;q=&amp;amp;esrc=s&amp;amp;source=web&amp;amp;cd=6&amp;amp;cad=rja&amp;amp;uact=8&amp;amp;ved=0ahUKEwjL37eA69rLAhVCXBoKHUkFBCkQFghDMAU&amp;amp;url=http%3A%2F%2Fwww.ugr.es%2F~lorente%2FAPUNTESMNM%2Fcapitulo5.pdf&amp;amp;usg=AFQjCNEXrijmpFscjHqL7WAoFqF_IXMoJA&quot;&gt;aproxime&lt;/a&gt;, solo está disponible el método &lt;a href=&quot;https://es.wikipedia.org/wiki/M%C3%ADnimos_cuadrados&quot;&gt;mínimos cuadrados&lt;/a&gt; para aproximación discreta.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;En esta primera entrada comentaré la idea general e implementación de 2 algoritmos de aprendizaje básicos (ambos redes neuronales de una única capa): &lt;a href=&quot;https://es.wikipedia.org/wiki/Perceptr%C3%B3n&quot;&gt;&lt;em&gt;Perceptron&lt;/em&gt;&lt;/a&gt; y &lt;a href=&quot;https://es.wikipedia.org/wiki/Adaline&quot;&gt;&lt;em&gt;Adaline&lt;/em&gt;&lt;/a&gt; se verán sus versiones &lt;a href=&quot;https://es.wikipedia.org/wiki/Clasificador_lineal&quot;&gt;lineales&lt;/a&gt; (aunque es fácilmente generalizable a funciones no lineales usando &lt;a href=&quot;https://es.wikipedia.org/wiki/M%C3%A1quinas_de_vectores_de_soporte#Funci.C3.B3n_Kernel&quot;&gt;&lt;em&gt;kernels&lt;/em&gt;&lt;/a&gt; en cierta parte del código que comentaré) haciendo uso de técnicas de optimización sin restricciones (en la siguiente entrada se verá como añadir restricciones y cotas). Por ello, para comprobar los resultados del entrenamiento y del test se utilizará un conjunto de muestras que variará dependiendo del algoritmo que esté comentando.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;perceptron&quot;&gt;Perceptron&lt;/h2&gt;
&lt;p&gt;Es un clasificador (en 2 clases {+1,-1} es el que trataremos) basado en funciones discriminantes lineales, el objetivo es encontrar un vector &lt;em&gt;θ&lt;/em&gt; que defina un &lt;a href=&quot;https://es.wikipedia.org/wiki/Hiperplano&quot;&gt;hiperplano&lt;/a&gt; separador p.e. una recta en 2D, que separe &lt;a href=&quot;http://photos1.blogger.com/blogger/1013/1515/320/SVMSeparacion.jpg&quot;&gt;correctamente&lt;/a&gt; el conjunto de muestras en 2 clases. Esto equivale a resolver un sistema de &lt;em&gt;N&lt;/em&gt; inecuaciones &lt;strong&gt;c&lt;/strong&gt;·&lt;strong&gt;θ&lt;/strong&gt;·&lt;strong&gt;x&lt;/strong&gt;≥&lt;strong&gt;0&lt;/strong&gt; (nótese la notación matricial, si se hace vector a vector, equivaldría a resolver &lt;a href=&quot;https://i.gyazo.com/e769171d48212f1ccd7d820184b9684d.png&quot;&gt;ésto&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;Ese sistema de &lt;em&gt;N&lt;/em&gt; inecuaciones con |θ| incógnitas es demasiado costoso de resolver mediante métodos directos (aunque podríamos haber empleado otros &lt;a href=&quot;https://es.wikipedia.org/wiki/M%C3%A9todo_iterativo&quot;&gt;métodos iterativos&lt;/a&gt; como los que iba a utilizar en &lt;a href=&quot;https://www.mcs.anl.gov/petsc/&quot;&gt;Petsc&lt;/a&gt;) y por ello, se opta por minimizar ésta &lt;a href=&quot;https://i.gyazo.com/5cccecf3f307437adb63609a97f0ed40.png&quot;&gt;función&lt;/a&gt; ( suma de las &lt;a href=&quot;https://i.gyazo.com/2a4626fe88c672270a4f180f6943aa9b.png&quot;&gt;distancias&lt;/a&gt; de cada muestra mal clasificada al hiperplano separador) equivalente a resolver el sistema original.
Este proceso se suele hacer de forma iterativa, calculando cada vez la suma de las distancias mencionada antes, modificando de acuerdo a esas distancias el valor de &lt;em&gt;θ&lt;/em&gt; y repitiendo el proceso hasta que &lt;em&gt;θ&lt;/em&gt; converge,varía muy poco, se acaban las iteraciones … .Sin embargo, podemos ahorrarnos la implementación del método iterativo haciendo uso del &lt;a href=&quot;http://docs.scipy.org/doc/scipy/reference/optimize.html&quot;&gt;módulo&lt;/a&gt; comentado antes, configurando correctamente la función objetivo a optimizar, las cotas, las restricciones (en entradas posteriores se verán, cuando se implementen máquinas de vectores de soporte &lt;a href=&quot;https://es.wikipedia.org/wiki/M%C3%A1quinas_de_vectores_de_soporte&quot;&gt;SVM&lt;/a&gt;), la tolerancia etc. Otra cosa a tener en cuenta es que Perceptron siempre converge si las muestras son linealmente separables.&lt;/p&gt;

&lt;p&gt;El código es el siguiente:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;#!/usr/bin/env python&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# -*- coding: utf-8 -*-&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#  Perceptron.py&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;scipy.optimize&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;minimize&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;Utils&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MatLoad&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;res&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;multiply&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array_equal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;res&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;=+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;=+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;9999999&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;res&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;res&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;XROWS&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;XCOLS&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;YROWS&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;minimize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fun&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;method&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;CG&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;classify&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inner&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;=+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inner&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__name__&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;__main__&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MatLoad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;X.np&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MatLoad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Y.np&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;XROWS&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;XCOLS&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;YROWS&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.0003&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.54&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;3.78&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;res&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; Detalles de convergencia &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;res&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;res&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Clase de [1,0,5]:&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;classify&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Para la implementación, se han desarrollado únicamente 4 funciones:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;f&lt;/em&gt;: es la función objetivo que he mencionado &lt;a href=&quot;https://gyazo.com/5cccecf3f307437adb63609a97f0ed40&quot;&gt;antes&lt;/a&gt;, se pasará como parámetro al solver de scipy para minimizar. Se hace un producto vector-matriz para calcular la parte &lt;strong&gt;c&lt;/strong&gt;·&lt;strong&gt;θ&lt;/strong&gt;·&lt;strong&gt;x&lt;/strong&gt; y se comprueba si todas las muestras están bien clasificadas, si lo están queremos que el proceso finalice luego devolvemos una distancia muy pequeña, si no lo están devolvemos la suma de las distancias de las muestras mal clasificadas al hiperplano separador.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;fit&lt;/em&gt;: se encarga de llamar al optimizador de &lt;em&gt;Scipy&lt;/em&gt; &lt;em&gt;minimize&lt;/em&gt;, utilizando el método del &lt;a href=&quot;https://es.wikipedia.org/wiki/M%C3%A9todo_del_gradiente_conjugado&quot;&gt;gradiente conjugado&lt;/a&gt; para entrenar el sistema. Podemos hacer uso de todos los demás &lt;a href=&quot;http://docs.scipy.org/doc/scipy/reference/optimize.html&quot;&gt;métodos&lt;/a&gt; implementados en la librería para el optimizador local &lt;em&gt;minimize&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;classify&lt;/em&gt;: una vez entrenado el sistema (obtenido el vector &lt;em&gt;θ&lt;/em&gt; adecuado), dada una nueva muestra &lt;em&gt;x&lt;/em&gt; permite clasificarla en la clase correcta según &lt;em&gt;θ&lt;/em&gt;. Para ello se calcula el producto escalar &lt;em&gt;θ&lt;/em&gt;·&lt;em&gt;x&lt;/em&gt; y se comprueba si está a un lado del espacio definido por el hiperplano separador o al otro ( &lt;em&gt;θ&lt;/em&gt;·&lt;em&gt;x&lt;/em&gt;≥0 , &lt;em&gt;θ&lt;/em&gt;·&lt;em&gt;x&lt;/em&gt;≤0 ).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;predict&lt;/em&gt;: dada una muestra &lt;em&gt;x&lt;/em&gt; permite hacer regresión una vez el sistema ha sido entrenado. Con ello, se predice el valor de &lt;em&gt;x&lt;/em&gt; dado el vector de parámetros &lt;em&gt;θ&lt;/em&gt;, en función del resultado del producto escalar &lt;em&gt;θ&lt;/em&gt;·&lt;em&gt;x&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Una vez se han definido las funciones necesarias, solo queda ver cómo se comporta el sistema entrenándolo y probándolo. Para ello, se ha hecho uso de un conjunto de muestras de juguete (en notación homogénea) junto con sus salidas, diseñado a mano:&lt;/p&gt;

&lt;p&gt;Muestras de entrenamiento: 3 muestras de 3 dimensiones dispuestas por columnas.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;nº&lt;/th&gt;
      &lt;th&gt;x1&lt;/th&gt;
      &lt;th&gt;x2&lt;/th&gt;
      &lt;th&gt;x3&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;d1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;d2&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;d3&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Salidas: salidas (clases) esperadas para cada una de las 3 muestras (+1 o -1):&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;nº&lt;/th&gt;
      &lt;th&gt;clase&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;x1&lt;/td&gt;
      &lt;td&gt;-1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;x2&lt;/td&gt;
      &lt;td&gt;+1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;x3&lt;/td&gt;
      &lt;td&gt;+1&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Dibujad las 2 últimas dimensiones de las muestras -d1 y d3 de la primera tabla- en un eje de coordenadas y veréis como encontráis rápidamente infinitas soluciones que separen las muestras en función de la salida (+1 o -1). Eso mismo hará el sistema, dar una posible solución de todas los hiperplanos (rectas en este caso) que sirven para separar las clases (en entradas posteriores, cuando se traten SVM, se podrá elegir entre el mejor de todos aquellos hiperplanos separadores según un criterio de distancia entre clases e hiperplano).&lt;/p&gt;

&lt;p&gt;Con ello (en el &lt;em&gt;main&lt;/em&gt; del &lt;em&gt;script&lt;/em&gt;) partimos de un vector &lt;em&gt;θ&lt;/em&gt; aleatorio (se suele inicializar a 0) y lo ajustamos mediante la función &lt;em&gt;fit&lt;/em&gt; para que clasifique correctamente las muestras, minimizando la función objetivo mencionada antes. Una vez finalice, obtenemos detalles sobre la finalización del proceso del &lt;em&gt;solver&lt;/em&gt; (valor de &lt;em&gt;f&lt;/em&gt; final, nº iteraciones realizadas, vector &lt;em&gt;θ&lt;/em&gt;, etc) y la salida de una muestra a clasificar [1,0,5], que corresponde al conjunto de muestras de entrenamiento y conocemos su clase (si hemos dibujado las coordenadas como dije) : -1. La salida del script ejemplo es:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Detalles de convergencia
 fun: 0.0
 jac: array([ 0.,  0.,  0.])
 message: &#39;Optimization terminated successfully.&#39;
    nfev: 10
     nit: 1
    njev: 2
  status: 0
 success: True
       x: array([ 1.9997,  4.46  , -0.78  ])

Clase de [1,0,5]: -1
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Que coincide con lo que esperábamos, observando que el método ha convergido en 2 iteraciones a la solución &lt;em&gt;θ&lt;/em&gt; = [ 1.9997,  4.46  , -0.78  ], además ha clasificado la muestra [1,0,5] que efectivamente pertenece a la clase -1.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;adaline&quot;&gt;Adaline&lt;/h2&gt;

&lt;p&gt;Adaline es una red neuronal de una capa utilizada en principalmente en regresión lineal. En este caso, el objetivo del aprendizaje es encontrar un vector &lt;em&gt;θ&lt;/em&gt; con el que se &lt;a href=&quot;https://i.gyazo.com/1b7efc98d8add5937953e722f18a57af.png&quot;&gt;minimice la diferencia entre &lt;em&gt;θ&lt;/em&gt;·&lt;em&gt;x&lt;/em&gt; y la salida esperada &lt;em&gt;y&lt;/em&gt; para toda muestra de entrenamiento&lt;/a&gt;. Del mismo modo que en Perceptron, se minimizará esta &lt;a href=&quot;https://i.gyazo.com/27cfe2a3959b5475029968f5fcc21b12.png&quot;&gt;función&lt;/a&gt; equivalente a resolver el sistema planteado (ahora no se tienen en cuenta solo las mal clasificadas, si no que se quiere minimizar las diferencias entre la RHS y la LHS del todo el conjunto de ecuaciones).En este caso no he encontrado ningún teorema sobre la convergencia excepto &lt;a href=&quot;https://i.gyazo.com/8abc9636f7f210ee276d53e4c043b794.png&quot;&gt;éste&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;El código es el siguiente:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;c&quot;&gt;#!/usr/bin/env python&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# -*- coding: utf-8 -*-&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#  Adaline.py&lt;/span&gt;

&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;scipy.optimize&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;minimize&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;Utils&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MatLoad&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;res&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;2.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;multiply&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;res&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;res&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
    

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;XROWS&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;XCOLS&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;YROWS&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;minimize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fun&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inner&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;classify&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inner&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;


&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__name__&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;__main__&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MatLoad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;X.np&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MatLoad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Y.np&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt; 
    &lt;span class=&quot;n&quot;&gt;XROWS&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;XCOLS&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;YROWS&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;XROWS&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;res&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt; Detalles de convergencia &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;res&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;res&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Clase de [1,0,5]:&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;classify&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;
    
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Es idéntico al del Perceptron, cambiando la función objetivo de &lt;a href=&quot;https://i.gyazo.com/5cccecf3f307437adb63609a97f0ed40.png&quot;&gt;&lt;em&gt;f&lt;/em&gt;&lt;/a&gt; a &lt;a href=&quot;https://i.gyazo.com/27cfe2a3959b5475029968f5fcc21b12.png&quot;&gt;&lt;em&gt;f’&lt;/em&gt;&lt;/a&gt;. Nótese que la minimización de las diferencias entre la LHS y RHS no garantiza resultados correctos en clasificación donde normalmente el conjunto de salidas posibles &lt;em&gt;Y&lt;/em&gt; es discreto.&lt;/p&gt;

&lt;p&gt;La salida del script es muy similar a la del caso anterior:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
 Detalles de convergencia

      fun: 7.76245804938178e-15
 hess_inv: array([[ 4.99999999, -1.        , -1.        ],
       [-1.        ,  0.26      ,  0.18      ],
       [-1.        ,  0.18      ,  0.24      ]])
      jac: array([ -1.30741529e-10,   3.51008111e-11,   1.42319490e-11])
  message: &#39;Optimization terminated successfully.&#39;
     nfev: 40
      nit: 5
     njev: 8
   status: 0
  success: True
        x: array([ 1.00000026,  0.19999994, -0.40000006])

Clase de [1,0,5]: -1

&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;El conjunto de muestras es el mismo que en el caso anterior, sin embargo, se puede observar que el vector &lt;em&gt;θ&lt;/em&gt; difiere del obtenido en Perceptron ya que al modificar la función objetivo se resuelve un problema diferente (éste, se podría haber resuelto utilizando la función &lt;a href=&quot;http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.optimize.leastsq.html#scipy.optimize.leastsq&quot;&gt;&lt;em&gt;leastsq&lt;/em&gt;&lt;/a&gt; del módulo &lt;em&gt;optimize&lt;/em&gt;).&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Hasta aquí lo que quería comentar, hay muchísima documentación sobre todo lo que he ido contando y he intentado enlazarlo y simplificarlo en la medida de lo posible para que sea comprensible. En próximas partes comentaré la implementación de máquinas de vectores de soporte &lt;a href=&quot;https://es.wikipedia.org/wiki/M%C3%A1quinas_de_vectores_de_soporte&quot;&gt;SVM&lt;/a&gt;, &lt;a href=&quot;https://en.wikipedia.org/wiki/Support_vector_machine#Soft-margin&quot;&gt;con&lt;/a&gt; y sin márgenes blandos (para ver una introducción la clasificación no lineal) y la aplicación de funciones &lt;a href=&quot;https://en.wikipedia.org/wiki/Positive-definite_kernel&quot;&gt;kernel&lt;/a&gt; para que los clasificadores implementados sepan discriminar de formas más complejas.&lt;/p&gt;

&lt;p&gt;Dejo el &lt;a href=&quot;https://github.com/Indeseables/indeseables.github.io/tree/master/_codigos/Entrada_1&quot;&gt;código disponible hasta el momento&lt;/a&gt; en el &lt;a href=&quot;https://github.com/Indeseables/indeseables.github.io&quot;&gt;repositorio&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Cualquier comentario:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Email: indeseables [at] gmail [dot] com&lt;/li&gt;
  &lt;li&gt;Twitter: &lt;a href=&quot;http://twitter.com/&quot;&gt;@indeseables&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Issues: &lt;a href=&quot;https://github.com/Indeseables/indeseables.github.io/issues&quot;&gt;#Issues&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Autor:&lt;/strong&gt; &lt;a href=&quot;https://github.com/overxfl0w&quot;&gt;&lt;em&gt;J.G&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Si tienes alguna duda, comentario o sugerencia, notifícanos a través de &lt;a href=&quot;mailto:indeseables.git@gmail.com&quot;&gt;indeseables.git@gmail.com&lt;/a&gt;
 y si te ha gustado la entrada puedes &lt;a href=&quot;https://twitter.com/intent/tweet?url=http://indeseables.github.io/2016/03/25/implementacion-alternativa-algoritmos-aprendizaje/&amp;amp;text=Implementación alternativa de algoritmos de aprendizaje (parte I).&amp;amp;via=indeseables!&quot; target=&quot;_blank&quot;&gt; compartirla!.&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;
</content>
 </entry>
 
 <entry>
   <title>What's Indeseables?</title>
   <link href="http://indeseables.github.io/2016/03/24/WhatsIndeseables/"/>
   <updated>2016-03-24T00:00:00+01:00</updated>
   <id>http://indeseables.github.io/2016/03/24/WhatsIndeseables</id>
   <content type="html">&lt;p&gt;&lt;a href=&quot;http://indeseables.github.io/&quot;&gt;Indeseables&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;fecuoosipqcedaraoanoeolsrmryiaiiedatbuatesnravnreiaeanrceinussnuaeuoribspndolndriaooiiiuudnaesucmipeauaemnsdlnhrnsyrcmdzrljrenarauaeeninitpbgstdgssozius
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;hr /&gt;

&lt;p&gt;Si tienes alguna duda, comentario o sugerencia, notifícanos a través de &lt;a href=&quot;mailto:indeseables.git@gmail.com&quot;&gt;indeseables.git@gmail.com&lt;/a&gt;
 y si te ha gustado la entrada puedes &lt;a href=&quot;https://twitter.com/intent/tweet?url=http://indeseables.github.io/2016/03/24/WhatsIndeseables/&amp;amp;text=What&#39;s Indeseables?&amp;amp;via=indeseables!&quot; target=&quot;_blank&quot;&gt; compartirla!.&lt;/a&gt;&lt;/p&gt;

&lt;hr /&gt;
</content>
 </entry>
 

</feed>
